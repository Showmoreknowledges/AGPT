"""
Rerank GTC candidates with an LLM using node texts.

Usage example:
    python rerank_with_llm.py \
      --candidates_path results/gtc_candidates.json \
      --texts_path data/douban_node_texts.json \
      --src_layer layer1 \
      --tgt_layer layer2 \
      --provider openai \
      --model gpt-4.1-mini \
      --output_json results/reranked_mapping_gpt4.1.json
"""

import argparse
import json
import os
import os.path as osp
import re
from typing import Dict, List, Tuple

from tqdm import tqdm


# ========== 1. è¿™é‡Œæ˜¯ä½ è¦æ”¹çš„ Prompt æ¨¡æ¿ ==========

def build_prompt(src_text: str, tgt_text: str):
    """
    æ„é€ ç»™ LLM çš„å®Œæ•´ promptã€‚
    ğŸ‘‰ å¦‚æœä½ æƒ³æ”¹æ¨¡æ¿ï¼Œåªéœ€è¦æ”¹è¿™ä¸ªå‡½æ•°é‡Œçš„å†…å®¹ã€‚
    """
    template = f"""
You are helping to align nodes between two related graphs.

Each node is described with structural and cross-layer information.

[Source node description]
{src_text}

[Target node description]
{tgt_text}

Task:
On a scale from 0 to 10, how likely is it that the source node
and the target node correspond to the same real-world entity?

Respond with only a single number between 0 and 10.
"""
    return template.strip()



def call_llm(provider: str, model: str, prompt: str) -> str:
    """
    æ ¹æ® provider è°ƒç”¨ä¸åŒçš„åç«¯ã€‚
    éœ€è¦ä½ åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ç›¸åº”çš„ API_KEYã€‚
    """
    provider = provider.lower()

    if provider == "openai":
        # ä½¿ç”¨ OpenAI å®˜æ–¹æ¥å£
        from openai import OpenAI
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        return resp.choices[0].message.content.strip()

    elif provider == "openrouter":
        # é€šè¿‡ OpenRouter è°ƒç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹ï¼ˆå¦‚ deepseekã€llama ç­‰ï¼‰
        from openai import OpenAI
        client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
        )
        resp = client.chat.completions.create(
            model=model,  # ä¾‹å¦‚ "deepseek/deepseek-chat"
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        return resp.choices[0].message.content.strip()

    elif provider == "deepseek":
        # ç›´æ¥è°ƒç”¨ DeepSeek å®˜æ–¹æ¥å£
        from openai import OpenAI
        client = OpenAI(
            base_url="https://api.deepseek.com",
            api_key=os.getenv("DEEPSEEK_API_KEY"),
        )
        resp = client.chat.completions.create(
            model=model,  # ä¾‹å¦‚ "deepseek-chat"
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        return resp.choices[0].message.content.strip()

    elif provider == "hf_local":
        # æœ¬åœ° HuggingFace æ¨¡å‹ï¼ˆå¦‚æœ¬åœ° LLaMAï¼‰
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        pipe = pipeline(
            "text-generation",
            model=model,            # æœ¬åœ°æˆ–ç¼“å­˜è·¯å¾„
            device_map="auto",
        )
        out = pipe(prompt, max_new_tokens=128, do_sample=False)[0]["generated_text"]
        return out.strip()

    else:
        raise ValueError(f"Unknown provider: {provider}")


def parse_score_from_response(text: str):
    """
    ä» LLM è¿”å›çš„æ–‡æœ¬ä¸­è§£æå‡ºä¸€ä¸ª 0-10 çš„åˆ†æ•°ã€‚
    ç®€å•åšæ³•ï¼šæå–ç¬¬ä¸€ä¸ªæµ®ç‚¹/æ•´æ•°ã€‚
    """
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç±»ä¼¼ 7, 7.5, 10, 0.0 è¿™æ ·çš„æ•°å­—
    match = re.search(r"[-+]?\d+(\.\d+)?", text)
    if not match:
        # æ‰¾ä¸åˆ°æ•°å­—æ—¶ï¼Œä¿å®ˆç»™ä¸€ä¸ªä¸­é—´å€¼
        return 5.0
    try:
        score = float(match.group(0))
    except ValueError:
        return 5.0
    # è£å‰ªåˆ° [0, 10]
    score = max(0.0, min(10.0, score))
    return score



def load_candidates(path: str):
    """
    æœŸæœ› JSON æ ¼å¼:
    {
        "0": [3, 5, 10],
        "1": [4, 7],
        ...
    }
    å³: æºèŠ‚ç‚¹ -> å€™é€‰ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨
    """
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    mapping: Dict[int, List[int]] = {}
    for k, v_list in data.items():
        u = int(k)
        mapping[u] = [int(x) for x in v_list]
    return mapping


def load_node_texts(path: str):
    """
    æœŸæœ› JSON æ ¼å¼ä¸ä½  generate_text è„šæœ¬è¾“å‡ºä¸€è‡´:
    {
        "layer1": {
            "0": "text ...",
            "1": "text ...",
            ...
        },
        "layer2": {
            "0": "text ...",
            ...
        }
    }
    """
    with open(path, "r", encoding="utf-8") as f:
        raw = json.load(f)
    texts: Dict[str, Dict[int, str]] = {}
    for layer_name, node_map in raw.items():
        texts[layer_name] = {}
        for node_id_str, text in node_map.items():
            texts[layer_name][int(node_id_str)] = text
    return texts



def rerank_candidates(
    candidates: Dict[int, List[int]],
    texts: Dict[str, Dict[int, str]],
    src_layer: str,
    tgt_layer: str,
    provider: str,
    model: str,
    max_candidates_per_src: int = 0,
) -> Tuple[Dict[int, List[int]], Dict[int, Dict[int, float]]]:
    """
    å¯¹æ¯ä¸ªæºèŠ‚ç‚¹ u å’Œå®ƒçš„å€™é€‰åˆ—è¡¨ vï¼Œç”¨ LLM æ‰“åˆ†å¹¶æ’åºã€‚

    è¿”å›:
        reranked_mapping: {u: [v1, v2, ...] æŒ‰å¾—åˆ†ä»é«˜åˆ°ä½}
        scores: {u: {v: score}}
    """
    src_texts = texts[src_layer]
    tgt_texts = texts[tgt_layer]

    reranked_mapping: Dict[int, List[int]] = {}
    scores: Dict[int, Dict[int, float]] = {}

    all_src_nodes = sorted(candidates.keys())

    for u in tqdm(all_src_nodes, desc="LLM reranking"):
        cand_vs = candidates[u]
        if max_candidates_per_src > 0:
            cand_vs = cand_vs[:max_candidates_per_src]

        src_text = src_texts.get(u, "")
        if not src_text:
            # æ²¡æœ‰æ–‡æœ¬æ—¶ï¼Œç»™ä¸€ä¸ªå ä½æè¿°
            src_text = f"Node {u} in layer {src_layer}."

        scores[u] = {}
        for v in cand_vs:
            tgt_text = tgt_texts.get(v, "")
            if not tgt_text:
                tgt_text = f"Node {v} in layer {tgt_layer}."

            prompt = build_prompt(src_text, tgt_text)
            try:
                response = call_llm(provider, model, prompt)
                score = parse_score_from_response(response)
            except Exception as e:
                print(f"[warn] LLM call failed for u={u}, v={v}: {e}")
                # å¤±è´¥æ—¶ç»™ä¸€ä¸ªä¸­ç­‰åˆ†
                score = 5.0

            scores[u][v] = score

        # æ ¹æ®å¾—åˆ†æ’åº
        sorted_vs = sorted(scores[u].keys(), key=lambda x: scores[u][x], reverse=True)
        reranked_mapping[u] = sorted_vs

    return reranked_mapping, scores