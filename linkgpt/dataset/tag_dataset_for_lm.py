import random
import sys
from typing import Dict, List, Tuple

import tqdm
import torch
import dgl
import datasets
import numpy as np

class TAGDatasetForLM():
    '''
    1. Initiate a dataset by `dataset_for_lm = TAGDatasetForLM(nid2data, nid_pair_list)`
    2. Generate edge split by `dataset_for_lm.generate_edge_split()`
    '''
    def __init__(self, nid2data: Dict[str, Dict], nid_pair_list: List[Tuple[str]], text_field: str=None, longer_text_field: str=None):
        """
        Here "nid" means "node ID", and its type is str.
        "nid" is the the ID used on the original dataset, like ASIN (e.g., "B00AYP7KPO") for Amazon Co-purchasing dataset.
        """
        self.text_field = text_field
        self.longer_text_field = longer_text_field
        
        if hasattr(nid2data, "keys"):
            self.nid_list = list(nid2data.keys())
        else:
            self.nid_list = list(range(nid2data.size(0)))
            
        self.nid2gnid = {nid: i for i, nid in enumerate(self.nid_list)}
        
        self.data_list = [nid2data[nid] for nid in self.nid_list]
        self.pair_list = [(self.nid2gnid[src], self.nid2gnid[tgt]) for src, tgt in nid_pair_list]
        
        gnid2neighbors = {gnid: [] for gnid, _ in enumerate(self.data_list)}
        for src, dst in self.pair_list:
            gnid2neighbors[src].append(dst)
        self.gnid2neighbors = gnid2neighbors
        
        # to be generated by manually calling generate_edge_split
        self.edge_split = None
        self.gnid2neighbors_train = None
        
    def get_neighbors(self, gnid: int):
        return self.gnid2neighbors[gnid]
    
    def __getitem__(self, gnid: int):
        return self.data_list[gnid]
    
    def __len__(self):
        return len(self.data_list)
    
    def generate_edge_split_with_splitted_pair_list(
        self,
        num_neg_dst: int,
        train_pair_list: List[Tuple[int]],
        valid_pair_list: List[Tuple[int]],
        test_pair_list: List[Tuple[int]],
        verbose: bool=True,
    ):
        pair_list = train_pair_list + valid_pair_list + test_pair_list
        dst_set = set([p[1] for p in pair_list])
        
        def sample_neg_dst(src: int):
            pos_dst_set = set(self.get_neighbors(src))
            while True:
                result = random.sample(range(len(self.data_list)), num_neg_dst)
                if all([i not in pos_dst_set for i in result]):
                    return result

        edge_split = {}
        edge_split['train'] = {
            'source_node': [pair[0] for pair in train_pair_list],
            'target_node': [pair[1] for pair in train_pair_list]
        }
        edge_split['valid'] = {
            'source_node': [pair[0] for pair in valid_pair_list],
            'target_node': [pair[1] for pair in valid_pair_list],
            'target_node_neg': [sample_neg_dst(pair[0]) for pair in tqdm.tqdm(valid_pair_list, disable=not verbose)]
        }
        edge_split['test'] = {
            'source_node': [pair[0] for pair in test_pair_list],
            'target_node': [pair[1] for pair in test_pair_list],
            'target_node_neg': [sample_neg_dst(pair[0]) for pair in tqdm.tqdm(test_pair_list, disable=not verbose)]
        }
        
        self.edge_split = edge_split
    
    def generate_edge_split(self, num_neg_dst: int, verbose: bool=True, train_ratio: float=0.9, valid_ratio: bool=0.05):
        """
        Generate edge split for training, validation, and testing.
        num_neg_dst: the number of negative target nodes to sample for each src node. 
        In this paper, we set it to 150 for LinkGPT w/o retrieval and 1800 for LinkGPT w/ retrieval.
        """
        if verbose:
            print('Generating edge split...')
            print(f'Num of negative dst per src: {num_neg_dst}')
        pair_list = self.pair_list
        random.shuffle(pair_list)
        num_train = int(len(pair_list) * train_ratio)
        num_valid = int(len(pair_list) * valid_ratio)
        num_test = len(pair_list) - num_valid - num_train
        
        train_pair_list = pair_list[:num_train]
        valid_pair_list = pair_list[num_train:num_train + num_valid]
        test_pair_list = pair_list[num_train + num_valid:]
        
        self.generate_edge_split_with_splitted_pair_list(num_neg_dst, train_pair_list, valid_pair_list, test_pair_list, verbose)
        self.generate_gnid2neighbors_train()
        
    def generate_gnid2neighbors_train(self):
        """
        Generate a dictionary that maps a gnid to its neighbors in the training set.
        """
        gnid2neighbors_train = {gnid: [] for gnid, _ in enumerate(self.data_list)}
        # since LLM does not need validation, we combine the valid and train set
        train_src_list = self.edge_split['train']['source_node'] + self.edge_split['valid']['source_node']
        train_dst_list = self.edge_split['train']['target_node'] + self.edge_split['valid']['target_node']
        for src, dst in zip(train_src_list, train_dst_list):
            gnid2neighbors_train[src].append(dst)
        self.gnid2neighbors_train = gnid2neighbors_train
        
    def get_neighbors_in_training_set(self, gnid: int):
        return self.gnid2neighbors_train[gnid]
    

def tag_dataset_for_lm_to_dgl_graph(dataset_for_lm: TAGDatasetForLM, device: str='cpu', include_valid: bool=True):
    """
    Convert a :class:`TAGDatasetForLM` object to a DGL graph.

    The original implementation always produced a homogeneous graph. For
    multi-relational data we optionally build a heterograph with one edge type
    per relation. The relation identifiers are expected to be provided in the
    ``edge_split`` dictionaries via the ``relation_type`` (or ``edge_type``)
    field. When the field is absent we fall back to the homogeneous behaviour
    to preserve backward compatibility.
    """

    def _cat(values: List[torch.Tensor]) -> torch.Tensor:
        return torch.cat(values, dim=0) if len(values) > 1 else values[0]
    
    num_nodes = len(dataset_for_lm)
    edge_split = dataset_for_lm.edge_split
    
    train_src = torch.tensor(edge_split['train']['source_node'], dtype=torch.long)
    train_dst = torch.tensor(edge_split['train']['target_node'], dtype=torch.long)
    src_tensors = [train_src]
    dst_tensors = [train_dst]

    relation_field = None
    for candidate in ('relation_type', 'relation_types', 'edge_type'):
        if candidate in edge_split['train']:
            relation_field = candidate
            break

    relation_tensors: List[torch.Tensor] = []
    if relation_field is not None:
        relation_tensors.append(torch.tensor(edge_split['train'][relation_field], dtype=torch.long))

    if include_valid and 'valid' in edge_split:
        valid_src = torch.tensor(edge_split['valid']['source_node'], dtype=torch.long)
        valid_dst = torch.tensor(edge_split['valid']['target_node'], dtype=torch.long)
        src_tensors.append(valid_src)
        dst_tensors.append(valid_dst)
        if relation_field is not None and relation_field in edge_split['valid']:
            relation_tensors.append(torch.tensor(edge_split['valid'][relation_field], dtype=torch.long))

    src_all = _cat(src_tensors)
    dst_all = _cat(dst_tensors)

    if relation_field is None or len(relation_tensors) == 0:
        g = dgl.graph((src_all, dst_all), num_nodes=num_nodes).to(device)
        g._linkgpt_is_hetero = False  # type: ignore[attr-defined]
        return g

    relation_all = _cat(relation_tensors)
    if relation_all.numel() != src_all.numel():
        raise ValueError(
            "Relation type list must have the same length as the edge list. "
            f"Got {relation_all.numel()} relation labels for {src_all.numel()} edges."
        )

    hetero_edges = {}
    unique_rel_ids = torch.unique(relation_all).tolist()
    for rel_id in unique_rel_ids:
        mask = relation_all == rel_id
        rel_name = f'rel_{int(rel_id)}'
        hetero_edges[('node', rel_name, 'node')] = (
            src_all[mask],
            dst_all[mask],
        )

    hetero_graph = dgl.heterograph(hetero_edges, num_nodes_dict={'node': num_nodes})
    for rel_id in unique_rel_ids:
        rel_name = f'rel_{int(rel_id)}'
        num_edges = hetero_graph.num_edges(rel_name)
        hetero_graph.edges[('node', rel_name, 'node')].data['relation_type'] = (
            torch.full((num_edges,), int(rel_id), dtype=torch.long)
        )

    hetero_graph = hetero_graph.to(device)
    hetero_graph._linkgpt_is_hetero = True  # type: ignore[attr-defined]
    hetero_graph._linkgpt_primary_ntype = 'node'  # type: ignore[attr-defined]
    return hetero_graph


def load_npz_to_TAGDatasetForLM(npz_path: str, text_field: str = 'text', longer_text_field: str = None,
                                feature_key: str = 'x1', pair_key: str = 'pos_pairs') -> TAGDatasetForLM:
    """
    Helper to load a typical multilayer/graph .npz (like Douban.npz / ACM-DBLP.npz)
    and convert it to a `TAGDatasetForLM` instance.

    Assumptions and behaviour:
    - By default it looks for node textual features under `feature_key` (e.g. 'x1').
      If that key is missing it will try to pick the first key that starts with 'x'.
    - Positive alignment pairs are expected in `pair_key` (default 'pos_pairs').
    - Node features can be an ndarray (dtype string/object), a list, or a dict mapping nid->text.
      The function will produce `nid2data` mapping integer node ids to dicts with `text_field`.
    - Returned `TAGDatasetForLM` will use integer node ids 0..N-1 as keys (strings are also accepted
      by the class, but integers are more natural when the .npz uses index-based arrays).

    This function intentionally keeps behaviour simple; if your .npz uses a different layout
    (multi-layer offsets, complex id namespaces), we can extend the loader accordingly.
    """
    data = np.load(npz_path, allow_pickle=True)

    # choose feature key
    if feature_key not in data.files:
        xs = [k for k in data.files if k.startswith('x')]
        feature_key = xs[0] if xs else None

    if feature_key is None:
        raise ValueError(f"No feature key found in {npz_path} (tried 'x1' and keys starting with 'x')")

    raw_feat = data[feature_key]

    nid2data = {}
    # ndarray of textual data or object dtype
    if isinstance(raw_feat, np.ndarray):
        if raw_feat.shape == ():
            # scalar container
            raw_feat = raw_feat.item()

        if isinstance(raw_feat, np.ndarray) and raw_feat.dtype.kind in {'U', 'S', 'O'}:
            # array-like of texts
            for i, txt in enumerate(raw_feat.tolist()):
                nid2data[i] = {text_field: txt}
        elif raw_feat.ndim >= 1 and raw_feat.dtype.kind not in {'U', 'S', 'O'}:
            # numeric feature matrix: store under a different key
            for i, row in enumerate(raw_feat):
                nid2data[i] = {'feat': row}
        else:
            # fallback to list conversion
            try:
                for i, txt in enumerate(list(raw_feat)):
                    nid2data[i] = {text_field: txt}
            except Exception:
                raise TypeError(f"Unsupported feature array format for key '{feature_key}' in {npz_path}")
    elif isinstance(raw_feat, (list, tuple)):
        for i, txt in enumerate(raw_feat):
            nid2data[i] = {text_field: txt}
    elif isinstance(raw_feat, dict):
        # dict mapping node id -> text or features
        for k, v in raw_feat.items():
            try:
                nid = int(k)
            except Exception:
                nid = k
            nid2data[nid] = {text_field: v}
    else:
        raise TypeError(f"Unsupported feature type for key '{feature_key}': {type(raw_feat)}")

    # load pair list
    if pair_key in data.files:
        pairs = data[pair_key]
    elif 'pos_pairs' in data.files:
        pairs = data['pos_pairs']
    else:
        raise ValueError(f"No pair list found in {npz_path} (tried '{pair_key}' and 'pos_pairs')")

    pair_list = []
    arr = np.asarray(pairs)
    if arr.ndim == 1 and arr.dtype == object:
        # array of pairs in object dtype
        for p in arr.tolist():
            if len(p) >= 2:
                pair_list.append((int(p[0]), int(p[1])))
    else:
        for p in arr.reshape(-1, 2):
            pair_list.append((int(p[0]), int(p[1])))

    return TAGDatasetForLM(nid2data, pair_list, text_field=text_field, longer_text_field=longer_text_field)