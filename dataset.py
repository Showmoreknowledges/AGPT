import os
import argparse
import pickle
import json
import random
from typing import Dict, Optional, Sequence, Tuple, Union

import numpy as np
import torch
from torch.utils.data import Dataset

try:
    from torch_sparse import SparseTensor  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    SparseTensor = None


class MultiLayerGraphDataset(Dataset):
    """
    åŠ è½½å¹¶å°è£…å¤šå±‚ç½‘ç»œï¼ˆå«é”šç‚¹å¯¹ï¼‰ï¼Œç”¨äº LinkGPT å‰ç½®æ•°æ®å¤„ç†ã€‚
    æ”¯æŒå­—æ®µï¼š
        - edge_index1, edge_index2ï¼šå›¾çš„è¾¹ç´¢å¼•
        - x1, x2ï¼šèŠ‚ç‚¹ç‰¹å¾æˆ–æ–‡æœ¬ï¼ˆå¯ä¸ºç©ºï¼‰
        - pos_pairs, test_pairsï¼šè®­ç»ƒä¸æµ‹è¯•èŠ‚ç‚¹å¯¹
    """
    def __init__(self, npz_path):
        super().__init__()
        data = np.load(npz_path, allow_pickle=True)

        self.edge_index1 = torch.from_numpy(data["edge_index1"]).long()
        self.edge_index2 = torch.from_numpy(data["edge_index2"]).long()

        # èŠ‚ç‚¹ç‰¹å¾ï¼ˆæˆ–æ–‡æœ¬ï¼‰
        self.x1 = data["x1"].item() if data["x1"].shape == () else data["x1"]
        self.x2 = data["x2"].item() if data["x2"].shape == () else data["x2"]
        if self.x1 is None:
            self.x1 = torch.zeros((int(self.edge_index1.max()) + 1, 1))
        if self.x2 is None:
            self.x2 = torch.zeros((int(self.edge_index2.max()) + 1, 1))

        self.pos_pairs = torch.from_numpy(data["pos_pairs"]).long()
        self.test_pairs = torch.from_numpy(data["test_pairs"]).long()

        self.num_nodes_1 = int(self.edge_index1.max()) + 1
        self.num_nodes_2 = int(self.edge_index2.max()) + 1

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {npz_path}")
        print(f"å›¾1: {self.num_nodes_1} ä¸ªèŠ‚ç‚¹, {self.edge_index1.shape[1]} æ¡è¾¹")
        print(f"å›¾2: {self.num_nodes_2} ä¸ªèŠ‚ç‚¹, {self.edge_index2.shape[1]} æ¡è¾¹")
        print(f"è®­ç»ƒå¯¹: {self.pos_pairs.shape[0]}, æµ‹è¯•å¯¹: {self.test_pairs.shape[0]}")

    def __len__(self):
        return len(self.pos_pairs)

    def __getitem__(self, idx):
        src, tgt = self.pos_pairs[idx]
        return {"src_id": src.item(), "tgt_id": tgt.item()}


def merge_graphs(edge_index1, edge_index2, x1=None, x2=None):
    def to_tensor(x):
        if x is None:
            return None
        if isinstance(x, np.ndarray):
            return torch.from_numpy(x).float()
        if isinstance(x, torch.Tensor):
            return x.float()
        raise TypeError(f"Unsupported feature type: {type(x)}")

    x1 = to_tensor(x1)
    x2 = to_tensor(x2)

    num_nodes_1 = int(edge_index1.max()) + 1
    num_nodes_2 = int(edge_index2.max()) + 1
    offset = num_nodes_1

    mapping_g2_to_merged = {i: i + offset for i in range(num_nodes_2)}
    edge_index2_shifted = edge_index2 + offset
    merged_edge_index = torch.cat([edge_index1, edge_index2_shifted], dim=1)

    if x1 is None:
        x1 = torch.zeros((num_nodes_1, 1))
    if x2 is None:
        x2 = torch.zeros((num_nodes_2, 1))
    merged_x = torch.cat([x1, x2], dim=0)

    print(f"âœ… åˆå¹¶å®Œæˆ: æ€»èŠ‚ç‚¹ {num_nodes_1 + num_nodes_2}, "
          f"æ€»è¾¹ {merged_edge_index.shape[1]}")
    return merged_edge_index, merged_x, mapping_g2_to_merged, num_nodes_1, num_nodes_2, num_nodes_1 + num_nodes_2


def process_node_features_for_cgtp(x1, x2, num_nodes_1, num_nodes_2, mapping_g2_to_merged):
    gnid2text, merged_features = None, None

    def is_textual(x):
        if x is None:
            return False
        if isinstance(x, np.ndarray) and x.dtype.type is np.str_:
            return True
        if isinstance(x, (list, tuple)):
            return all(isinstance(i, str) for i in x)
        if isinstance(x, dict):
            return all(isinstance(v, str) for v in x.values())
        return False

    if is_textual(x1) or is_textual(x2):
        gnid2text = {}
        if isinstance(x1, dict):
            for nid, text in x1.items():
                gnid2text[int(nid)] = text
        elif isinstance(x1, (list, tuple)):
            for nid, text in enumerate(x1):
                gnid2text[nid] = text
        if isinstance(x2, dict):
            for orig_id, text in x2.items():
                gnid2text[mapping_g2_to_merged[int(orig_id)]] = text
        elif isinstance(x2, (list, tuple)):
            for orig_id, text in enumerate(x2):
                gnid2text[mapping_g2_to_merged[orig_id]] = text
        print(f"âœ… å·²ç”Ÿæˆ gnid2text å­—å…¸ï¼Œå…± {len(gnid2text)} æ¡æ–‡æœ¬èŠ‚ç‚¹")
    else:
        def to_tensor(x, n):
            if x is None:
                return torch.zeros((n, 1))
            if isinstance(x, np.ndarray):
                return torch.from_numpy(x).float()
            if isinstance(x, torch.Tensor):
                return x.float()
            raise TypeError(f"Unsupported feature type: {type(x)}")

        merged_features = torch.cat([to_tensor(x1, num_nodes_1), to_tensor(x2, num_nodes_2)], dim=0)
        print(f"âœ… å·²ç”Ÿæˆ merged_features ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶: {merged_features.shape}")
    return gnid2text, merged_features


def process_alignment_pairs(pos_pairs, test_pairs, num_nodes_1):
    if not isinstance(pos_pairs, torch.Tensor):
        pos_pairs = torch.tensor(pos_pairs, dtype=torch.long)
    if not isinstance(test_pairs, torch.Tensor):
        test_pairs = torch.tensor(test_pairs, dtype=torch.long)
    train_pairs_merged, test_pairs_merged = pos_pairs.clone(), test_pairs.clone()
    train_pairs_merged[:, 1] += num_nodes_1
    test_pairs_merged[:, 1] += num_nodes_1
    print(f"âœ… èŠ‚ç‚¹å¯¹æ˜ å°„å®Œæˆ: train={train_pairs_merged.shape}, test={test_pairs_merged.shape}")
    return train_pairs_merged, test_pairs_merged


class TAGDatasetForLM(Dataset):
    def __init__(self, merged_edge_index, gnid2text=None, merged_features=None):
        super().__init__()
        self.edge_index = merged_edge_index
        self.gnid2text = gnid2text
        self.features = merged_features
        self.num_nodes = (
            merged_features.shape[0] if merged_features is not None else len(gnid2text)
        )
        self.data_list = []
        for i in range(self.num_nodes):
            node_info = {"node_id": i, "neighbors": self._get_neighbors(i)}
            if self.gnid2text is not None:
                node_info["text"] = self.gnid2text.get(i, "")
            if self.features is not None:
                node_info["feature"] = self.features[i]
            self.data_list.append(node_info)
        print(f"âœ… TAGDatasetForLM: å…± {self.num_nodes} ä¸ªèŠ‚ç‚¹æ ·æœ¬")

    def __len__(self):
        return self.num_nodes

    def __getitem__(self, idx):
        return self.data_list[idx]

    def _get_neighbors(self, idx):
        src_mask = self.edge_index[0] == idx
        dst_mask = self.edge_index[1] == idx
        neighbors = torch.cat([self.edge_index[1][src_mask], self.edge_index[0][dst_mask]]).unique().tolist()
        return neighbors

    def get_neighbors_in_training_set(self, gnid):
        return self._get_neighbors(gnid)


class AlignmentDataset(Dataset):
    """
    ç½‘ç»œå¯¹é½æ•°æ®é›†ã€‚

    æ—¢å…¼å®¹æ—§ç‰ˆçš„ `AlignmentDataset(node_emb_path, pair_path, ppr_path)` è°ƒç”¨æ–¹å¼ï¼Œ
    ä¹Ÿæ”¯æŒç›´æ¥ä¼ å…¥å·²åŠ è½½çš„å¼ é‡ï¼Œä¸”ä¼šåœ¨åˆå§‹åŒ–é˜¶æ®µç”Ÿæˆè´Ÿæ ·æœ¬å¹¶ç¼“å­˜ï¼Œæ–¹ä¾¿ Trainer ç›´æ¥è¿­ä»£ã€‚
    """

    def __init__(
        self,
        *legacy_paths: str,
        node_embeddings: Optional[torch.Tensor] = None,
        pairs: Optional[torch.Tensor] = None,
        ppr: Optional[Union[torch.Tensor, "SparseTensor", Dict[Tuple[int, int], float]]] = None,
        node_emb_path: Optional[str] = None,
        pair_path: Optional[str] = None,
        ppr_path: Optional[str] = None,
        neg_ratio: int = 1,
        seed: int = 42,
        shuffle: bool = True,
    ) -> None:
        if legacy_paths:
            if len(legacy_paths) != 3:
                raise ValueError("Legacy mode expects (node_emb_path, pair_path, ppr_path) as three positional arguments.")
            node_emb_path, pair_path, ppr_path = legacy_paths  # type: ignore[misc]

        self.node_emb = self._load_tensor(node_embeddings, node_emb_path, "node_embeddings")
        self.pairs = self._load_tensor(pairs, pair_path, "pairs", dtype=torch.long)
        self.ppr_lookup = self._prepare_ppr(self._load_optional_ppr(ppr, ppr_path))

        if self.pairs.dim() != 2 or self.pairs.size(1) != 2:
            raise ValueError("`pairs` should be a 2D tensor with shape [N, 2].")

        self.pos_pairs = self.pairs.long().cpu()
        self.neg_ratio = max(int(neg_ratio), 0)
        self.seed = seed
        self.shuffle = shuffle
        self.rng = random.Random(seed)

        self.target_candidates = self._infer_target_candidates(self.pos_pairs)
        self.neg_pairs = self._build_negative_pairs()

        pos_labels = torch.ones(self.pos_pairs.size(0), dtype=torch.float32)
        if self.neg_pairs.numel() > 0:
            neg_labels = torch.zeros(self.neg_pairs.size(0), dtype=torch.float32)
            self.all_pairs = torch.cat([self.pos_pairs, self.neg_pairs], dim=0)
            self.labels = torch.cat([pos_labels, neg_labels], dim=0)
        else:
            self.all_pairs = self.pos_pairs
            self.labels = pos_labels

        if self.shuffle and len(self.all_pairs) > 1:
            generator = torch.Generator().manual_seed(self.seed)
            perm = torch.randperm(len(self.all_pairs), generator=generator)
            self.all_pairs = self.all_pairs[perm]
            self.labels = self.labels[perm]

        print(
            f"âœ… AlignmentDataset å‡†å¤‡å®Œæˆ: æ­£æ ·æœ¬ {self.pos_pairs.size(0)}, "
            f"è´Ÿæ ·æœ¬ {self.neg_pairs.size(0)}, åµŒå…¥ç»´åº¦ {self.node_emb.shape[1]}"
        )

    def __len__(self) -> int:
        return self.all_pairs.size(0)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        u, v = self.all_pairs[idx]
        label = self.labels[idx]

        sample = {
            "u_id": u.long(),
            "v_id": v.long(),
            "u_emb": self.node_emb[u].float(),
            "v_emb": self.node_emb[v].float(),
            "labels": label,
        }

        sample["ppr_score"] = self._lookup_ppr(int(u), int(v))
        return sample

    def resample_negatives(self) -> None:
        """é‡æ–°é‡‡æ ·è´Ÿæ ·æœ¬ï¼Œå¯åœ¨æ¯ä¸ª epoch å‰è°ƒç”¨ä»¥æå‡è®­ç»ƒç¨³å®šæ€§ã€‚"""
        self.neg_pairs = self._build_negative_pairs()
        pos_labels = torch.ones(self.pos_pairs.size(0), dtype=torch.float32)
        if self.neg_pairs.numel() > 0:
            neg_labels = torch.zeros(self.neg_pairs.size(0), dtype=torch.float32)
            self.all_pairs = torch.cat([self.pos_pairs, self.neg_pairs], dim=0)
            self.labels = torch.cat([pos_labels, neg_labels], dim=0)
        else:
            self.all_pairs = self.pos_pairs
            self.labels = pos_labels
        if self.shuffle and len(self.all_pairs) > 1:
            generator = torch.Generator().manual_seed(self.seed)
            perm = torch.randperm(len(self.all_pairs), generator=generator)
            self.all_pairs = self.all_pairs[perm]
            self.labels = self.labels[perm]

    @staticmethod
    def _load_tensor(
        tensor: Optional[torch.Tensor],
        path: Optional[str],
        name: str,
        dtype: Optional[torch.dtype] = None,
    ) -> torch.Tensor:
        if tensor is None:
            if path is None:
                raise ValueError(f"`{name}` must be provided either as a tensor or a file path.")
            tensor = torch.load(path, map_location="cpu")
        if not isinstance(tensor, torch.Tensor):
            raise TypeError(f"`{name}` must be a torch.Tensor, but got {type(tensor)}.")
        tensor = tensor.detach().cpu()
        if dtype is not None:
            tensor = tensor.to(dtype)
        return tensor

    @staticmethod
    def _load_optional_ppr(
        ppr: Optional[Union[torch.Tensor, "SparseTensor", Dict[Tuple[int, int], float]]],
        path: Optional[str],
    ) -> Optional[Union[torch.Tensor, "SparseTensor", Dict[Tuple[int, int], float]]]:
        if ppr is None and path is not None:
            ppr = torch.load(path, map_location="cpu")
        return ppr

    def _prepare_ppr(
        self,
        ppr: Optional[Union[torch.Tensor, "SparseTensor", Dict[Tuple[int, int], float]]],
    ) -> Optional[Union[torch.Tensor, Dict[Tuple[int, int], float]]]:
        if ppr is None:
            return None
        if isinstance(ppr, dict):
            return {(int(k[0]), int(k[1])): float(v) for k, v in ppr.items()}
        if isinstance(ppr, torch.Tensor):
            if ppr.is_sparse:
                coalesced = ppr.coalesce()
                indices = coalesced.indices().t().tolist()
                values = coalesced.values().tolist()
                return {(int(i), int(j)): float(v) for (i, j), v in zip(indices, values)}
            return ppr.float().cpu()
        if SparseTensor is not None and isinstance(ppr, SparseTensor):
            row, col, val = ppr.coo()
            return {(int(i), int(j)): float(v) for i, j, v in zip(row.tolist(), col.tolist(), val.tolist())}
        raise TypeError(f"Unsupported PPR type: {type(ppr)}")

    def _infer_target_candidates(self, pairs: torch.Tensor) -> Sequence[int]:
        target_ids = torch.unique(pairs[:, 1]).cpu().tolist()
        return target_ids

    def _build_negative_pairs(self) -> torch.Tensor:
        if self.neg_ratio <= 0 or len(self.target_candidates) <= 1:
            return torch.zeros((0, 2), dtype=torch.long)

        neg_samples = []
        for src, tgt in self.pos_pairs.tolist():
            for _ in range(self.neg_ratio):
                neg_tgt = self._sample_negative_target(int(tgt))
                neg_samples.append((int(src), neg_tgt))
        if not neg_samples:
            return torch.zeros((0, 2), dtype=torch.long)
        return torch.tensor(neg_samples, dtype=torch.long)

    def _sample_negative_target(self, positive_tgt: int) -> int:
        neg_tgt = positive_tgt
        while neg_tgt == positive_tgt:
            neg_tgt = self.rng.choice(self.target_candidates)
        return neg_tgt

    def _lookup_ppr(self, u: int, v: int) -> torch.Tensor:
        if self.ppr_lookup is None:
            return torch.tensor(0.0, dtype=torch.float32)
        if isinstance(self.ppr_lookup, torch.Tensor):
            return self.ppr_lookup[u, v].view(1).float()
        return torch.tensor(self.ppr_lookup.get((u, v), 0.0), dtype=torch.float32).view(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help="å¤šå±‚ç½‘ç»œæ•°æ®çš„ .npz æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="/root/autodl-tmp/prepared_data", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    dataset = MultiLayerGraphDataset(args.data_path)

    merged_edge_index, merged_x, mapping_g2_to_merged, n1, n2, n_total = merge_graphs(
        dataset.edge_index1, dataset.edge_index2, dataset.x1, dataset.x2
    )
    gnid2text, merged_features = process_node_features_for_cgtp(
        dataset.x1, dataset.x2, n1, n2, mapping_g2_to_merged
    )
    train_pairs_merged, test_pairs_merged = process_alignment_pairs(dataset.pos_pairs, dataset.test_pairs, n1)
    tag_dataset = TAGDatasetForLM(merged_edge_index, gnid2text, merged_features)

    # === æ–¹æ¡ˆ2: æ¸…ç† features å†ä¿å­˜å¯¹è±¡ ===
    output_dir = args.output_dir

    # 1ï¸âƒ£ ä¿å­˜åŸºç¡€æ•°æ®
    torch.save(merged_edge_index, os.path.join(output_dir, "merged_edge_index.pt"))
    torch.save(train_pairs_merged, os.path.join(output_dir, "train_pairs_merged.pt"))
    torch.save(test_pairs_merged, os.path.join(output_dir, "test_pairs_merged.pt"))

    # 2ï¸âƒ£ ä¿å­˜å¤§çŸ©é˜µç‹¬ç«‹æ–‡ä»¶
    if merged_features is not None and merged_features.numel() > 0:
        torch.save(merged_features, os.path.join(output_dir, "merged_features.pt"))
        print(f"âœ… merged_features å·²å•ç‹¬ä¿å­˜ ({merged_features.shape})")

    # 3ï¸âƒ£ æ¸…ç©ºç‰¹å¾å†ä¿å­˜ dataset å¯¹è±¡
    tag_dataset.features = None
    with open(os.path.join(output_dir, "dataset_for_lm.pkl"), "wb") as f:
        pickle.dump(tag_dataset, f)
    print(f"âœ… dataset_for_lm.pkl ä¿å­˜å®Œæˆï¼ˆä¸å«ç‰¹å¾çŸ©é˜µï¼‰")

    # 4ï¸âƒ£ å…¶ä»–å¯é€‰ä¿å­˜
    if gnid2text is not None:
        with open(os.path.join(output_dir, "gnid2text.json"), "w", encoding="utf-8") as f:
            json.dump(gnid2text, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ¯ æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {os.path.abspath(args.output_dir)}")
    print("æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œå¯ç›´æ¥è¿›å…¥ CGTP é¢„è®­ç»ƒé˜¶æ®µã€‚")
