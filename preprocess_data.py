import os
import argparse
import pickle
import json
import torch
from typing import Dict, List, Tuple, Optional


from dataset.tag_dataset_for_lm import TAGDatasetForLM
# å¯¼å…¥ dataset.py ä¸­å®šä¹‰çš„ç±»å’Œå‡½æ•°
from Mul_dataset import (
    MultiLayerGraphDataset,
    merge_graphs,
    process_node_features_for_cgtp,
    process_alignment_pairs,
)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", type=str, required=True, help="å¤šå±‚ç½‘ç»œæ•°æ®")
    parser.add_argument("--data_path", type=str, required=True, help="å¤šå±‚ç½‘ç»œæ•°æ®çš„ .npz æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="/root/autodl-tmp/prepared_data", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    os.makedirs(f"{args.output_dir}/{args.data}", exist_ok=True)
    dataset = MultiLayerGraphDataset(args.data_path)

    merged_edge_index, _, layer_offsets, num_nodes_per_layer, total_num_nodes = merge_graphs(
        dataset.edge_indices, dataset.features
    )
    gnid2text, merged_features = process_node_features_for_cgtp(
        dataset.features, num_nodes_per_layer, layer_offsets
    )
    # å°†å¯¹é½å¯¹æ˜ å°„åˆ°åˆå¹¶åçš„å…¨å±€èŠ‚ç‚¹ id ç©ºé—´
    train_pairs_merged, test_pairs_merged = process_alignment_pairs(
        dataset.pos_pairs, dataset.test_pairs, layer_offsets
    )
    
    def _tensor_pairs_to_list(pairs: torch.Tensor) -> List[Tuple[int, int]]:
        if pairs.numel() == 0:
            return []
        return [(int(src), int(tgt)) for src, tgt in pairs.tolist()]

    combined_pairs: List[Tuple[int, int]] = []
    for tensor in (train_pairs_merged, test_pairs_merged):
        combined_pairs.extend(_tensor_pairs_to_list(tensor))
    
    # å®šä¹‰æ¯ä¸ªæ­£æ ·æœ¬å¯¹åº”çš„è´Ÿæ ·æœ¬æ•°é‡
    NUM_NEG_SAMPLES = 100 

    def _sample_neg_targets(pos_pairs: torch.Tensor, num_neg: int) -> torch.Tensor:
        """
        ä¸ºç»™å®šçš„æ­£æ ·æœ¬å¯¹é‡‡æ · num_neg ä¸ªè´Ÿç›®æ ‡èŠ‚ç‚¹ã€‚
        è¿”å›ä¸€ä¸ªåŒ…å« num_pos_edges * num_neg ä¸ªå…ƒç´ çš„å¹³é“ºå¼ é‡ã€‚
        """
        if pos_pairs.numel() == 0:
            return torch.tensor([], dtype=torch.long)
            
        num_pos_edges = pos_pairs.size(0)
        num_nodes = total_num_nodes
        
        # éšæœºé‡‡æ · num_pos_edges * num_neg ä¸ªèŠ‚ç‚¹ ID ä½œä¸ºè´Ÿç›®æ ‡
        # é‡‡æ ·å½¢çŠ¶: (num_pos_edges, num_neg)ï¼Œç„¶åå¹³é“ºæˆä¸€ç»´åˆ—è¡¨
        neg_targets_candidates = torch.randint(
            low=0, 
            high=num_nodes, 
            size=(num_pos_edges, num_neg), 
            dtype=torch.long
        )
        
        return neg_targets_candidates

    # ç”Ÿæˆè´Ÿæ ·æœ¬
    test_neg_targets_merged = _sample_neg_targets(test_pairs_merged, NUM_NEG_SAMPLES)
    print(f"âœ… å·²ä¸ºéªŒè¯/æµ‹è¯•é›†é‡‡æ · {test_neg_targets_merged.numel()} ä¸ªè´Ÿç›®æ ‡èŠ‚ç‚¹ ({NUM_NEG_SAMPLES}x)ã€‚")

    node_records: Dict[int, Dict[str, object]] = {}
    for node_id in range(total_num_nodes):
        node_entry: Dict[str, object] = {"node_id": node_id}
        if gnid2text is not None:
            node_entry["text"] = gnid2text.get(node_id, "")
        else:
            node_entry["text"] = ""
        node_records[node_id] = node_entry

    tag_dataset = TAGDatasetForLM(node_records, combined_pairs)
    tag_dataset.features = merged_features
    tag_dataset.gnid2text = gnid2text if gnid2text is not None else None
    tag_dataset.layer_offsets = layer_offsets
    tag_dataset.num_nodes_per_layer = num_nodes_per_layer
    tag_dataset.total_num_nodes = total_num_nodes

    def _build_edge_split_section(
        pairs: torch.Tensor, neg_targets: Optional[torch.Tensor] = None # <- ç¡®ä¿æ·»åŠ äº† neg_targets
    ) -> Dict[str, object]:
        pairs_cpu = pairs.detach().cpu()
        src_list: List[int] = pairs_cpu[:, 0].tolist() if pairs_cpu.numel() > 0 else []
        tgt_list: List[int] = pairs_cpu[:, 1].tolist() if pairs_cpu.numel() > 0 else []
        section: Dict[str, object] = {
            "source_node": src_list,
            "target_node": tgt_list,
        }
        section["edge"] = pairs_cpu.numpy()
        
        # ç¡®ä¿è¿™éƒ¨åˆ†ä»£ç ä¹Ÿå·²æ·»åŠ 
        if neg_targets is not None and neg_targets.numel() > 0:
            section["target_node_neg"] = neg_targets.detach().cpu().tolist()
            
        return section

    print("âœ… æ­£åœ¨å°†åˆå¹¶åçš„å¯¹é½é“¾æ¥å’Œè´Ÿæ ·æœ¬æ³¨å…¥ 'dataset_for_lm.edge_split'...")
    test_split_section = _build_edge_split_section(test_pairs_merged, neg_targets=test_neg_targets_merged)
    
    tag_dataset.edge_split = {
        "train": _build_edge_split_section(train_pairs_merged), # è®­ç»ƒé›†æ— éœ€è´Ÿæ ·æœ¬
        "valid": test_split_section,
        "test": test_split_section,
    }
    tag_dataset.generate_gnid2neighbors_train()

    # === æ¸…ç† features å†ä¿å­˜å¯¹è±¡ ===
    output_dir = f"{args.output_dir}/{args.data}"

    # 1ï¸âƒ£ ä¿å­˜åŸºç¡€æ•°æ®
    torch.save(merged_edge_index, os.path.join(output_dir, "merged_edge_index.pt"))
    torch.save(train_pairs_merged, os.path.join(output_dir, "train_pairs_merged.pt"))
    torch.save(test_pairs_merged, os.path.join(output_dir, "test_pairs_merged.pt"))

    # 2ï¸âƒ£ ä¿å­˜å¤§çŸ©é˜µç‹¬ç«‹æ–‡ä»¶
    if merged_features is not None and merged_features.numel() > 0:
        torch.save(merged_features, os.path.join(output_dir, "merged_features.pt"))
        print(f"âœ… merged_features å·²å•ç‹¬ä¿å­˜ ({merged_features.shape})")

    # 3ï¸âƒ£ æ¸…ç©ºç‰¹å¾å†ä¿å­˜ dataset å¯¹è±¡
    tag_dataset.features = None
    tag_dataset.gnid2text = None
    with open(os.path.join(output_dir, "dataset_for_lm.pkl"), "wb") as f:
        pickle.dump(tag_dataset, f)
    print(f"âœ… dataset_for_lm.pkl ä¿å­˜å®Œæˆï¼ˆå·²æ³¨å…¥å¯¹é½é“¾æ¥ï¼‰") 

    # 4ï¸âƒ£ å…¶ä»–å¯é€‰ä¿å­˜
    if gnid2text is not None:
        with open(os.path.join(output_dir, "gnid2text.json"), "w", encoding="utf-8") as f:
            json.dump(gnid2text, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ¯ æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {os.path.abspath(args.output_dir)}")
    print("æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œå¯ç›´æ¥è¿›å…¥ CGTP é¢„è®­ç»ƒé˜¶æ®µã€‚")
